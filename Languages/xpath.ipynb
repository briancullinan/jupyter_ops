{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xpath\n",
    "\n",
    "Taken from: https://github.com/emory-libraries/eulxml/tree/master/eulxml/xpath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the core\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Core XPath parsing glue.\n",
    "\n",
    "This module builds a lexer and parser for XPath expressions for import into\n",
    "eulxml.xpath. To understand how this module builds the lexer and parser, it\n",
    "is helpful to understand how the `ply <http://www.dabeaz.com/ply/>`_ module\n",
    "works.\n",
    "\n",
    "Note that most client applications will import htese objects from\n",
    "eulxml.xpath, not directly from here.\"\"\"\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import re\n",
    "from ply import lex, yacc\n",
    "import tempfile\n",
    "\n",
    "from eulxml.xpath import lexrules\n",
    "from eulxml.xpath import parserules\n",
    "from eulxml.xpath.ast import serialize\n",
    "\n",
    "__all__ = ['lexer', 'parser', 'parse', 'serialize']\n",
    "\n",
    "# build the lexer. This will generate a lextab.py in the eulxml.xpath\n",
    "# directory. Unfortunately, xpath requires some wonky lexing.\n",
    "# Per http://www.w3.org/TR/xpath/#exprlex :\n",
    "#  1 If there is a preceding token and the preceding token is not one of @,\n",
    "#    ::, (, [, , or an Operator, then a * must be recognized as a\n",
    "#    MultiplyOperator and an NCName must be recognized as an OperatorName.\n",
    "#  2 If the character following an NCName (possibly after intervening\n",
    "#    ExprWhitespace) is (, then the token must be recognized as a NodeType\n",
    "#    or a FunctionName.\n",
    "#  3 If the two characters following an NCName (possibly after intervening\n",
    "#    ExprWhitespace) are ::, then the token must be recognized as an\n",
    "#    AxisName.\n",
    "#  4 Otherwise, the token must not be recognized as a MultiplyOperator, an\n",
    "#    OperatorName, a NodeType, a FunctionName, or an AxisName.\n",
    "#\n",
    "# To implement this, we create a wrapper class that extends token() for the\n",
    "# described lookahead/lookback lexing, and we dynamically set the lexer's\n",
    "# __class__ to this wrapper. That's pretty weird and ugly, but Python allows\n",
    "# it. If you can find a prettier solution to the problem then I welcome a\n",
    "# fix.\n",
    "\n",
    "OPERATOR_FORCERS = set([\n",
    "    # @, ::, (, [\n",
    "    'ABBREV_AXIS_AT', 'AXIS_SEP', 'OPEN_PAREN', 'OPEN_BRACKET',\n",
    "    # Operators: OperatorName\n",
    "    'AND_OP', 'OR_OP', 'MOD_OP', 'DIV_OP', 'MULT_OP',\n",
    "    # Operators: MultiplyOperator\n",
    "    'PATH_SEP',\n",
    "    # Operators: /, //, |, +, -\n",
    "    'ABBREV_PATH_SEP', 'UNION_OP', 'PLUS_OP', 'MINUS_OP',\n",
    "    # Operators: =. !=, <, <=, >, >=\n",
    "    'EQUAL_OP', 'REL_OP',\n",
    "\n",
    "    # Also need to add : . Official XPath lexing rules are in terms of\n",
    "    # QNames, but we produce QNames in the parse layer. We need to include :\n",
    "    # here to force foo:div to be a single step, otherwise that last div\n",
    "    # would be interpreted as an operator (where standard xpath would just\n",
    "    # call it part of the qname)\n",
    "    'COLON',\n",
    "])\n",
    "\n",
    "NODE_TYPES = set(['comment', 'text', 'processing-instruction', 'node'])\n",
    "\n",
    "\n",
    "class LexerWrapper(lex.Lexer):\n",
    "    def token(self):\n",
    "        tok = lex.Lexer.token(self)\n",
    "        if tok is not None:\n",
    "            if tok.type == 'STAR_OP':\n",
    "                if self.last is not None and self.last.type not in OPERATOR_FORCERS:\n",
    "                    # first half of point 1\n",
    "                    tok.type = 'MULT_OP'\n",
    "\n",
    "            if tok.type == 'NCNAME':\n",
    "                if self.last is not None and self.last.type not in OPERATOR_FORCERS:\n",
    "                    # second half of point 1\n",
    "                    operator = lexrules.operator_names.get(tok.value, None)\n",
    "                    if operator is not None:\n",
    "                        tok.type = operator\n",
    "                else:\n",
    "                    next = self.peek()\n",
    "                    if next is not None:\n",
    "                        if next.type == 'OPEN_PAREN':\n",
    "                            # point 2\n",
    "                            if tok.value in NODE_TYPES:\n",
    "                                tok.type = 'NODETYPE'\n",
    "                            else:\n",
    "                                tok.type = 'FUNCNAME'\n",
    "                        elif next.type == 'AXIS_SEP':\n",
    "                            # point 3\n",
    "                            tok.type = 'AXISNAME'\n",
    "\n",
    "        self.last = tok\n",
    "        return tok\n",
    "\n",
    "    def peek(self):\n",
    "        clone = self.clone()\n",
    "        return clone.token()\n",
    "\n",
    "# try to build the lexer with cached lex table generation. this will fail if\n",
    "# the user doesn't have write perms on the source directory. in that case,\n",
    "# try again without lex table generation.\n",
    "lexdir = os.path.dirname(lexrules.__file__)\n",
    "lexer = None\n",
    "try:\n",
    "    lexer = lex.lex(module=lexrules, optimize=1, outputdir=lexdir,\n",
    "        reflags=re.UNICODE)\n",
    "except IOError as e:\n",
    "    import errno\n",
    "    if e.errno != errno.EACCES:\n",
    "        raise\n",
    "if lexer is None:\n",
    "    lexer = lex.lex(module=lexrules, reflags=re.UNICODE)\n",
    "# then dynamically rewrite the lexer class to use the wonky override logic\n",
    "# above\n",
    "lexer.__class__ = LexerWrapper\n",
    "lexer.last = None\n",
    "\n",
    "# build the parser. This will generate a parsetab.py in the eulxml.xpath\n",
    "# directory. Unlike lex, though, this just logs a complaint when it fails\n",
    "# (contrast lex's explosion). Other than that, it's much less exciting\n",
    "# than the lexer wackiness.\n",
    "parsedir = os.path.dirname(parserules.__file__)\n",
    "# By default, store generated parse files with the code\n",
    "# If we don't have write permission, put them in the configured tempdir\n",
    "if (not os.access(parsedir, os.W_OK)):\n",
    "    parsedir = tempfile.gettempdir()\n",
    "parser = yacc.yacc(module=parserules, outputdir=parsedir, debug=0)\n",
    "\n",
    "\n",
    "def parse(xpath):\n",
    "    '''Parse an xpath.'''\n",
    "    # Expose the parse method of the constructed parser,\n",
    "    # but explicitly specify the lexer created here,\n",
    "    # since otherwise parse will use the most-recently created lexer.\n",
    "    return parser.parse(xpath, lexer=lexer)\n",
    "\n",
    "\n",
    "def ptokens(s):\n",
    "    '''Lex a string as XPath tokens, and print each token as it is lexed.\n",
    "    This is used primarily for debugging. You probably don't want this\n",
    "    function.'''\n",
    "\n",
    "    lexer.input(s)\n",
    "    for tok in lexer:\n",
    "            print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''Abstract Syntax Tree nodes for parsed XPath.\n",
    "\n",
    "This module contains basic nodes for representing parsed XPath expressions.\n",
    "The parser provided by this module creates its parsed XPath representation\n",
    "from the classes defined in this module. Library callers will mostly not use\n",
    "this module directly, unless they need to produce XPath ASTs from scratch or\n",
    "perhaps introspect ASTs returned by the parser.\n",
    "'''\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import sys\n",
    "\n",
    "\n",
    "# python2/3 string type logic borrowed from six\n",
    "# NOTE: not importing six here because setup.py needs to generate\n",
    "# the parser at install time, when six installation is not yet available\n",
    "PY2 = sys.version_info[0] == 2\n",
    "PY3 = sys.version_info[0] == 3\n",
    "\n",
    "if PY3:\n",
    "    string_types = str\n",
    "else:\n",
    "    string_types = basestring\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'serialize',\n",
    "    'UnaryExpression',\n",
    "    'BinaryExpression',\n",
    "    'PredicatedExpression',\n",
    "    'AbsolutePath',\n",
    "    'Step',\n",
    "    'NameTest',\n",
    "    'NodeType',\n",
    "    'AbbreviatedStep',\n",
    "    'VariableReference',\n",
    "    'FunctionCall',\n",
    "    ]\n",
    "\n",
    "def serialize(xp_ast):\n",
    "    '''Serialize an XPath AST as a valid XPath expression.'''\n",
    "    return ''.join(_serialize(xp_ast))\n",
    "\n",
    "def _serialize(xp_ast):\n",
    "    '''Generate token strings which, when joined together, form a valid\n",
    "    XPath serialization of the AST.'''\n",
    "\n",
    "    if hasattr(xp_ast, '_serialize'):\n",
    "        for tok in xp_ast._serialize():\n",
    "            yield tok\n",
    "    elif isinstance(xp_ast, string_types):\n",
    "        # strings in serialized xpath needed to be quoted\n",
    "        # (e.g. for use in paths, comparisons, etc)\n",
    "        # using repr to quote them; for unicode, the leading\n",
    "        # u (u'') needs to be removed.\n",
    "        yield repr(xp_ast).lstrip('u')\n",
    "    else:\n",
    "        yield str(xp_ast)\n",
    "\n",
    "\n",
    "class UnaryExpression(object):\n",
    "\n",
    "    '''A unary XPath expression. Practially, this means -foo.'''\n",
    "\n",
    "    def __init__(self, op, right):\n",
    "        self.op = op\n",
    "        '''the operator used in the expression'''\n",
    "        self.right = right\n",
    "        '''the expression the operator is applied to'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s %s>' % (self.__class__.__name__,\n",
    "                self.op, serialize(self.right))\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield self.op\n",
    "        for tok in _serialize(self.right):\n",
    "            yield tok\n",
    "\n",
    "\n",
    "KEYWORDS = set(['or', 'and', 'div', 'mod'])\n",
    "class BinaryExpression(object):\n",
    "\n",
    "    '''Any binary XPath expression. a/b; a and b; a | b.'''\n",
    "\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        '''the left side of the binary expression'''\n",
    "        self.op = op\n",
    "        '''the operator of the binary expression'''\n",
    "        self.right = right\n",
    "        '''the right side of the binary expression'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s %s %s>' % (self.__class__.__name__,\n",
    "                serialize(self.left), self.op, serialize(self.right))\n",
    "\n",
    "    def _serialize(self):\n",
    "        for tok in _serialize(self.left):\n",
    "            yield tok\n",
    "\n",
    "        if self.op in KEYWORDS:\n",
    "            yield ' '\n",
    "            yield self.op\n",
    "            yield ' '\n",
    "        else:\n",
    "            yield self.op\n",
    "\n",
    "        for tok in _serialize(self.right):\n",
    "            yield tok\n",
    "\n",
    "\n",
    "class PredicatedExpression(object):\n",
    "\n",
    "    '''A filtered XPath expression. $var[1]; (a or b)[foo][@bar].'''\n",
    "\n",
    "    def __init__(self, base, predicates=None):\n",
    "        self.base = base\n",
    "        '''the base expression to be filtered'''\n",
    "        self.predicates = predicates or []\n",
    "        '''a list of filter predicates'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def append_predicate(self, pred):\n",
    "        self.predicates.append(pred)\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield '('\n",
    "        for tok in _serialize(self.base):\n",
    "            yield tok\n",
    "        yield ')'\n",
    "        for pred in self.predicates:\n",
    "            yield '['\n",
    "            for tok in _serialize(pred):\n",
    "                yield tok\n",
    "            yield ']'\n",
    "\n",
    "\n",
    "class AbsolutePath(object):\n",
    "\n",
    "    '''An absolute XPath path. /a/b/c; //a/ancestor:b/@c.'''\n",
    "\n",
    "    def __init__(self, op='/', relative=None):\n",
    "        self.op = op\n",
    "        '''the operator used to root the expression'''\n",
    "        self.relative = relative\n",
    "        '''the relative path after the absolute root operator'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.relative:\n",
    "            return '<%s %s %s>' % (self.__class__.__name__,\n",
    "                    self.op, serialize(self.relative))\n",
    "        else:\n",
    "            return '<%s %s>' % (self.__class__.__name__, self.op)\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield self.op\n",
    "        for tok in _serialize(self.relative):\n",
    "            yield tok\n",
    "\n",
    "\n",
    "class Step(object):\n",
    "\n",
    "    '''A single step in a relative path. a; @b; text(); parent::foo:bar[5].'''\n",
    "\n",
    "    def __init__(self, axis, node_test, predicates):\n",
    "        self.axis = axis\n",
    "        '''the step's axis, or @ or None if abbreviated or undefined'''\n",
    "        self.node_test = node_test\n",
    "        '''a NameTest or NodeType object describing the test represented'''\n",
    "        self.predicates = predicates\n",
    "        '''a list of predicates filtering the step'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        if self.axis == '@':\n",
    "            yield '@'\n",
    "        elif self.axis:\n",
    "            yield self.axis\n",
    "            yield '::'\n",
    "\n",
    "        for tok in self.node_test._serialize():\n",
    "            yield tok\n",
    "\n",
    "        for predicate in self.predicates:\n",
    "            yield '['\n",
    "            for tok in _serialize(predicate):\n",
    "                yield tok\n",
    "            yield ']'\n",
    "\n",
    "\n",
    "class NameTest(object):\n",
    "\n",
    "    '''An element name node test for a Step.'''\n",
    "\n",
    "    def __init__(self, prefix, name):\n",
    "        self.prefix = prefix\n",
    "        '''the namespace prefix used for the test, or None if unset'''\n",
    "        self.name = name\n",
    "        '''the node name used for the test, or *'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        if self.prefix:\n",
    "            yield self.prefix\n",
    "            yield ':'\n",
    "        yield self.name\n",
    "\n",
    "    def __str__(self):\n",
    "        return ''.join(self._serialize())\n",
    "\n",
    "class NodeType(object):\n",
    "\n",
    "    '''A node type node test for a Step.'''\n",
    "\n",
    "    def __init__(self, name, literal=None):\n",
    "        self.name = name\n",
    "        '''the node type name, such as node or text'''\n",
    "        self.literal = literal\n",
    "        '''the argument to the node specifier. XPath allows these only for\n",
    "        processing-instruction() node tests.'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield self.name\n",
    "        yield '('\n",
    "        if self.literal is not None:\n",
    "            for tok in _serialize(self.literal):\n",
    "                yield self.literal\n",
    "        yield ')'\n",
    "\n",
    "    def __str__(self):\n",
    "        return ''.join(self._serialize())\n",
    "\n",
    "class AbbreviatedStep(object):\n",
    "\n",
    "    '''An abbreviated XPath step. . or ..'''\n",
    "\n",
    "    def __init__(self, abbr):\n",
    "        self.abbr = abbr\n",
    "        '''the abbreviated step'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield self.abbr\n",
    "\n",
    "\n",
    "class VariableReference(object):\n",
    "\n",
    "    '''An XPath variable reference. $foo; $myns:foo.'''\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        '''a tuple (prefix, localname) containing the variable name'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        yield '$'\n",
    "        prefix, localname = self.name\n",
    "        if prefix:\n",
    "            yield prefix\n",
    "            yield ':'\n",
    "        yield localname\n",
    "\n",
    "\n",
    "class FunctionCall(object):\n",
    "\n",
    "    '''An XPath function call. foo(); my:foo(1); foo(1, 'a', $var).'''\n",
    "\n",
    "    def __init__(self, prefix, name, args):\n",
    "        self.prefix = prefix\n",
    "        '''the namespace prefix, or None if unspecified'''\n",
    "        self.name = name\n",
    "        '''the local function name'''\n",
    "        self.args = args\n",
    "        '''a list of argument expressions'''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s %s>' % (self.__class__.__name__,\n",
    "                serialize(self))\n",
    "\n",
    "    def _serialize(self):\n",
    "        if self.prefix:\n",
    "            yield self.prefix\n",
    "            yield ':'\n",
    "        yield self.name\n",
    "        yield '('\n",
    "        if self.args:\n",
    "            for tok in _serialize(self.args[0]):\n",
    "                yield tok\n",
    "\n",
    "            for arg in self.args[1:]:\n",
    "                yield ','\n",
    "                for tok in _serialize(arg):\n",
    "                    yield tok\n",
    "        yield ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"XPath lexing rules.\n",
    "\n",
    "To understand how this module works, it is valuable to have a strong\n",
    "understanding of the `ply <http://www.dabeaz.com/ply/>` module.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "operator_names = {\n",
    "    'or': 'OR_OP',\n",
    "    'and': 'AND_OP',\n",
    "    'div': 'DIV_OP',\n",
    "    'mod': 'MOD_OP',\n",
    "}\n",
    "\n",
    "tokens = [\n",
    "        'PATH_SEP',\n",
    "        'ABBREV_PATH_SEP',\n",
    "        'ABBREV_STEP_SELF',\n",
    "        'ABBREV_STEP_PARENT',\n",
    "        'AXIS_SEP',\n",
    "        'ABBREV_AXIS_AT',\n",
    "        'OPEN_PAREN',\n",
    "        'CLOSE_PAREN',\n",
    "        'OPEN_BRACKET',\n",
    "        'CLOSE_BRACKET',\n",
    "        'UNION_OP',\n",
    "        'EQUAL_OP',\n",
    "        'REL_OP',\n",
    "        'PLUS_OP',\n",
    "        'MINUS_OP',\n",
    "        'MULT_OP',\n",
    "        'STAR_OP',\n",
    "        'COMMA',\n",
    "        'LITERAL',\n",
    "        'FLOAT',\n",
    "        'INTEGER',\n",
    "        'NCNAME',\n",
    "        'NODETYPE',\n",
    "        'FUNCNAME',\n",
    "        'AXISNAME',\n",
    "        'COLON',\n",
    "        'DOLLAR',\n",
    "    ] + list(operator_names.values())\n",
    "\n",
    "t_PATH_SEP = r'/'\n",
    "t_ABBREV_PATH_SEP = r'//'\n",
    "t_ABBREV_STEP_SELF = r'\\.'\n",
    "t_ABBREV_STEP_PARENT = r'\\.\\.'\n",
    "t_AXIS_SEP = r'::'\n",
    "t_ABBREV_AXIS_AT = r'@'\n",
    "t_OPEN_PAREN = r'\\('\n",
    "t_CLOSE_PAREN = r'\\)'\n",
    "t_OPEN_BRACKET = r'\\['\n",
    "t_CLOSE_BRACKET = r'\\]'\n",
    "t_UNION_OP = r'\\|'\n",
    "t_EQUAL_OP = r'!?='\n",
    "t_REL_OP = r'[<>]=?'\n",
    "t_PLUS_OP = r'\\+'\n",
    "t_MINUS_OP = r'-'\n",
    "t_COMMA = r','\n",
    "t_COLON = r':'\n",
    "t_DOLLAR = r'\\$'\n",
    "t_STAR_OP = r'\\*'\n",
    "\n",
    "t_ignore = ' \\t\\r\\n'\n",
    "\n",
    "# NOTE: some versions of python cannot compile regular expressions that\n",
    "# contain unicode characters above U+FFFF, which are allowable in NCNames.\n",
    "# These characters can be used in Python 2.6.4, but can NOT be used in 2.6.2\n",
    "# (status in 2.6.3 is unknown).  The code below accounts for that and excludes\n",
    "# the higher character range if Python can't handle it.\n",
    "\n",
    "# Monster regex derived from:\n",
    "#  http://www.w3.org/TR/REC-xml/#NT-NameStartChar\n",
    "#  http://www.w3.org/TR/REC-xml/#NT-NameChar\n",
    "# EXCEPT:\n",
    "# Technically those productions allow ':'. NCName, on the other hand:\n",
    "#  http://www.w3.org/TR/REC-xml-names/#NT-NCName\n",
    "# explicitly excludes those names that have ':'. We implement this by\n",
    "# simply removing ':' from our regexes.\n",
    "\n",
    "# NameStartChar regex without characters about U+FFFF\n",
    "NameStartChar = r'[A-Z]|_|[a-z]|\\xc0-\\xd6]|[\\xd8-\\xf6]|[\\xf8-\\u02ff]|' + \\\n",
    "    r'[\\u0370-\\u037d]|[\\u037f-\\u1fff]|[\\u200c-\\u200d]|[\\u2070-\\u218f]|' + \\\n",
    "    r'[\\u2c00-\\u2fef]|[\\u3001-\\uD7FF]|[\\uF900-\\uFDCF]|[\\uFDF0-\\uFFFD]'\n",
    "# complete NameStartChar regex\n",
    "Full_NameStartChar = r'(' + NameStartChar + r'|[\\U00010000-\\U000EFFFF]' + r')'\n",
    "# additional characters allowed in NCNames after the first character\n",
    "NameChar_extras = r'[-.0-9\\xb7\\u0300-\\u036f\\u203f-\\u2040]'\n",
    "\n",
    "try:\n",
    "    import re\n",
    "    # test whether or not re can compile unicode characters above U+FFFF\n",
    "    re.compile(r'[\\U00010000-\\U00010001]')\n",
    "    # if that worked, then use the full ncname regex\n",
    "    NameStartChar = Full_NameStartChar\n",
    "except:\n",
    "    # if compilation failed, leave NameStartChar regex as is, which does not\n",
    "    # include the unicode character ranges above U+FFFF\n",
    "    pass\n",
    "\n",
    "NCNAME_REGEX = r'(' + NameStartChar + r')(' + \\\n",
    "                      NameStartChar + r'|' + NameChar_extras + r')*'\n",
    "\n",
    "NODE_TYPES = set(['comment', 'text', 'processing-instruction', 'node'])\n",
    "\n",
    "t_NCNAME = NCNAME_REGEX\n",
    "\n",
    "def t_LITERAL(t):\n",
    "    r\"\"\"\"[^\"]*\"|'[^']*'\"\"\"\n",
    "    t.value = t.value[1:-1]\n",
    "    return t\n",
    "\n",
    "def t_FLOAT(t):\n",
    "    r'\\d+\\.\\d*|\\.\\d+'\n",
    "    t.value = float(t.value)\n",
    "    return t\n",
    "\n",
    "def t_INTEGER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "def t_error(t):\n",
    "    raise TypeError(\"Unknown text '%s'\" % (t.value,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"XPath parsing rules.\n",
    "\n",
    "To understand how this module works, it is valuable to have a strong\n",
    "understanding of the `ply <http://www.dabeaz.com/ply/>` module.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from eulxml.xpath import ast\n",
    "from eulxml.xpath.lexrules import tokens\n",
    "\n",
    "precedence = (\n",
    "    ('left', 'OR_OP'),\n",
    "    ('left', 'AND_OP'),\n",
    "    ('left', 'EQUAL_OP'),\n",
    "    ('left', 'REL_OP'),\n",
    "    ('left', 'PLUS_OP', 'MINUS_OP'),\n",
    "    ('left', 'MULT_OP', 'DIV_OP', 'MOD_OP'),\n",
    "    ('right', 'UMINUS_OP'),\n",
    "    ('left', 'UNION_OP'),\n",
    ")\n",
    "\n",
    "#\n",
    "# basic expressions\n",
    "#\n",
    "\n",
    "def p_expr_boolean(p):\n",
    "    \"\"\"\n",
    "    Expr : Expr OR_OP Expr\n",
    "         | Expr AND_OP Expr\n",
    "         | Expr EQUAL_OP Expr\n",
    "         | Expr REL_OP Expr\n",
    "         | Expr PLUS_OP Expr\n",
    "         | Expr MINUS_OP Expr\n",
    "         | Expr MULT_OP Expr\n",
    "         | Expr DIV_OP Expr\n",
    "         | Expr MOD_OP Expr\n",
    "         | Expr UNION_OP Expr\n",
    "    \"\"\"\n",
    "    p[0] = ast.BinaryExpression(p[1], p[2], p[3])\n",
    "\n",
    "def p_expr_unary(p):\n",
    "    \"\"\"\n",
    "    Expr : MINUS_OP Expr %prec UMINUS_OP\n",
    "    \"\"\"\n",
    "    p[0] = ast.UnaryExpression(p[1], p[2])\n",
    "\n",
    "#\n",
    "# path expressions\n",
    "#\n",
    "\n",
    "def p_path_expr_binary(p):\n",
    "    \"\"\"\n",
    "    Expr : FilterExpr PATH_SEP RelativeLocationPath\n",
    "         | FilterExpr ABBREV_PATH_SEP RelativeLocationPath\n",
    "    \"\"\"\n",
    "    p[0] = ast.BinaryExpression(p[1], p[2], p[3])\n",
    "\n",
    "def p_path_expr_unary(p):\n",
    "    \"\"\"\n",
    "    Expr : RelativeLocationPath\n",
    "         | AbsoluteLocationPath\n",
    "         | AbbreviatedAbsoluteLocationPath\n",
    "         | FilterExpr\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "#\n",
    "# paths\n",
    "#\n",
    "\n",
    "def p_absolute_location_path_rootonly(p):\n",
    "    \"\"\"\n",
    "    AbsoluteLocationPath : PATH_SEP\n",
    "    \"\"\"\n",
    "    p[0] = ast.AbsolutePath(p[1])\n",
    "\n",
    "def p_absolute_location_path_subpath(p):\n",
    "    \"\"\"\n",
    "    AbsoluteLocationPath : PATH_SEP RelativeLocationPath\n",
    "    \"\"\"\n",
    "    p[0] = ast.AbsolutePath(p[1], p[2])\n",
    "\n",
    "def p_abbreviated_absolute_location_path(p):\n",
    "    \"\"\"\n",
    "    AbbreviatedAbsoluteLocationPath : ABBREV_PATH_SEP RelativeLocationPath\n",
    "    \"\"\"\n",
    "    p[0] = ast.AbsolutePath(p[1], p[2])\n",
    "\n",
    "def p_relative_location_path_simple(p):\n",
    "    \"\"\"\n",
    "    RelativeLocationPath : Step\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_relative_location_path_binary(p):\n",
    "    \"\"\"\n",
    "    RelativeLocationPath : RelativeLocationPath PATH_SEP Step\n",
    "                         | RelativeLocationPath ABBREV_PATH_SEP Step\n",
    "    \"\"\"\n",
    "    p[0] = ast.BinaryExpression(p[1], p[2], p[3])\n",
    "\n",
    "#\n",
    "# path steps\n",
    "#\n",
    "\n",
    "def p_step_nodetest(p):\n",
    "    \"\"\"\n",
    "    Step : NodeTest\n",
    "    \"\"\"\n",
    "    p[0] = ast.Step(None, p[1], [])\n",
    "\n",
    "def p_step_nodetest_predicates(p):\n",
    "    \"\"\"\n",
    "    Step : NodeTest PredicateList\n",
    "    \"\"\"\n",
    "    p[0] = ast.Step(None, p[1], p[2])\n",
    "\n",
    "def p_step_axis_nodetest(p):\n",
    "    \"\"\"\n",
    "    Step : AxisSpecifier NodeTest\n",
    "    \"\"\"\n",
    "    p[0] = ast.Step(p[1], p[2], [])\n",
    "\n",
    "def p_step_axis_nodetest_predicates(p):\n",
    "    \"\"\"\n",
    "    Step : AxisSpecifier NodeTest PredicateList\n",
    "    \"\"\"\n",
    "    p[0] = ast.Step(p[1], p[2], p[3])\n",
    "\n",
    "def p_step_abbrev(p):\n",
    "    \"\"\"\n",
    "    Step : ABBREV_STEP_SELF\n",
    "         | ABBREV_STEP_PARENT\n",
    "    \"\"\"\n",
    "    p[0] = ast.AbbreviatedStep(p[1])\n",
    "\n",
    "#\n",
    "# axis specifier\n",
    "#\n",
    "\n",
    "def p_axis_specifier_full(p):\n",
    "    \"\"\"\n",
    "    AxisSpecifier : AXISNAME AXIS_SEP\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_axis_specifier_abbrev(p):\n",
    "    \"\"\"\n",
    "    AxisSpecifier : ABBREV_AXIS_AT\n",
    "    \"\"\"\n",
    "    p[0] = '@'\n",
    "\n",
    "#\n",
    "# node test\n",
    "#\n",
    "\n",
    "def p_node_test_name_test(p):\n",
    "    \"\"\"\n",
    "    NodeTest : NameTest\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_node_test_type_simple(p):\n",
    "    \"\"\"\n",
    "    NodeTest : NODETYPE OPEN_PAREN CLOSE_PAREN\n",
    "    \"\"\"\n",
    "    # NOTE: Strictly speaking p[1] must come from a list of recognized\n",
    "    # NodeTypes. Since we don't actually do anything with them, we don't\n",
    "    # need to recognize them.\n",
    "    p[0] = ast.NodeType(p[1])\n",
    "\n",
    "def p_node_test_type_literal(p):\n",
    "    \"\"\"\n",
    "    NodeTest : NODETYPE OPEN_PAREN LITERAL CLOSE_PAREN\n",
    "    \"\"\"\n",
    "    # NOTE: Technically this only allows 'processing-instruction' for p[1].\n",
    "    # We'll go light on that restriction since we don't actually need it for\n",
    "    # processing.\n",
    "    p[0] = ast.NodeType(p[1], p[3])\n",
    "\n",
    "#\n",
    "# name test\n",
    "#\n",
    "\n",
    "def p_name_test_star(p):\n",
    "    \"\"\"\n",
    "    NameTest : STAR_OP\n",
    "    \"\"\"\n",
    "    p[0] = ast.NameTest(None, p[1])\n",
    "\n",
    "def p_name_test_prefix_star(p):\n",
    "    \"\"\"\n",
    "    NameTest : NCNAME COLON STAR_OP\n",
    "    \"\"\"\n",
    "    p[0] = ast.NameTest(p[1], p[3])\n",
    "\n",
    "def p_name_test_qname(p):\n",
    "    \"\"\"\n",
    "    NameTest : QName\n",
    "    \"\"\"\n",
    "    qname = p[1]\n",
    "    p[0] = ast.NameTest(qname[0], qname[1])\n",
    "\n",
    "\n",
    "#\n",
    "# qname\n",
    "#\n",
    "\n",
    "def p_qname_prefixed(p):\n",
    "    \"\"\"\n",
    "    QName : NCNAME COLON NCNAME\n",
    "    \"\"\"\n",
    "    p[0] = (p[1], p[3])\n",
    "\n",
    "def p_qname_unprefixed(p):\n",
    "    \"\"\"\n",
    "    QName : NCNAME\n",
    "    \"\"\"\n",
    "    p[0] = (None, p[1])\n",
    "\n",
    "def p_funcqname_prefixed(p):\n",
    "    \"\"\"\n",
    "    FuncQName : NCNAME COLON FUNCNAME\n",
    "    \"\"\"\n",
    "    p[0] = (p[1], p[3])\n",
    "\n",
    "def p_funcqname_unprefixed(p):\n",
    "    \"\"\"\n",
    "    FuncQName : FUNCNAME\n",
    "    \"\"\"\n",
    "    p[0] = (None, p[1])\n",
    "\n",
    "#\n",
    "# filter expressions\n",
    "#\n",
    "\n",
    "def p_filter_expr_simple(p):\n",
    "    \"\"\"\n",
    "    FilterExpr : VariableReference\n",
    "               | LITERAL\n",
    "               | Number\n",
    "               | FunctionCall\n",
    "    \"\"\"\n",
    "    # FIXME: | FunctionCall moved so as not to conflict with NodeTest :\n",
    "    # FunctionCall\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_filter_expr_grouped(p):\n",
    "    \"\"\"\n",
    "    FilterExpr : OPEN_PAREN Expr CLOSE_PAREN\n",
    "    \"\"\"\n",
    "    p[0] = p[2]\n",
    "\n",
    "def p_filter_expr_predicate(p):\n",
    "    \"\"\"\n",
    "    FilterExpr : FilterExpr Predicate\n",
    "    \"\"\"\n",
    "    if not hasattr(p[1], 'append_predicate'):\n",
    "        p[1] = ast.PredicatedExpression(p[1])\n",
    "    p[1].append_predicate(p[2])\n",
    "    p[0] = p[1]\n",
    "\n",
    "#\n",
    "# predicates\n",
    "#\n",
    "\n",
    "def p_predicate_list_single(p):\n",
    "    \"\"\"\n",
    "    PredicateList : Predicate\n",
    "    \"\"\"\n",
    "    p[0] = [p[1]]\n",
    "\n",
    "def p_predicate_list_recursive(p):\n",
    "    \"\"\"\n",
    "    PredicateList : PredicateList Predicate\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "    p[0].append(p[2])\n",
    "\n",
    "def p_predicate(p):\n",
    "    \"\"\"\n",
    "    Predicate : OPEN_BRACKET Expr CLOSE_BRACKET\n",
    "    \"\"\"\n",
    "    p[0] = p[2]\n",
    "\n",
    "#\n",
    "# variable\n",
    "#\n",
    "\n",
    "def p_variable_reference(p):\n",
    "    \"\"\"\n",
    "    VariableReference : DOLLAR QName\n",
    "    \"\"\"\n",
    "    p[0] = ast.VariableReference(p[2])\n",
    "\n",
    "#\n",
    "# number\n",
    "#\n",
    "\n",
    "def p_number(p):\n",
    "    \"\"\"\n",
    "    Number : FLOAT\n",
    "           | INTEGER\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "#\n",
    "# funcall\n",
    "#\n",
    "\n",
    "def p_function_call(p):\n",
    "    \"\"\"\n",
    "    FunctionCall : FuncQName FormalArguments\n",
    "    \"\"\"\n",
    "    # FIXME: This production also matches NodeType() or\n",
    "    # processing-instruction(\"foo\"), which are technically NodeTest\n",
    "    qname = p[1]\n",
    "    p[0] = ast.FunctionCall(qname[0], qname[1], p[2])\n",
    "\n",
    "def p_formal_arguments_empty(p):\n",
    "    \"\"\"\n",
    "    FormalArguments : OPEN_PAREN CLOSE_PAREN\n",
    "    \"\"\"\n",
    "    p[0] = []\n",
    "\n",
    "def p_formal_arguments_list(p):\n",
    "    \"\"\"\n",
    "    FormalArguments : OPEN_PAREN ArgumentList CLOSE_PAREN\n",
    "    \"\"\"\n",
    "    p[0] = p[2]\n",
    "\n",
    "def p_argument_list_single(p):\n",
    "    \"\"\"\n",
    "    ArgumentList : Expr\n",
    "    \"\"\"\n",
    "    p[0] = [p[1]]\n",
    "\n",
    "def p_argument_list_recursive(p):\n",
    "    \"\"\"\n",
    "    ArgumentList : ArgumentList COMMA Expr\n",
    "    \"\"\"\n",
    "    p[0] = p[1]\n",
    "    p[0].append(p[3])\n",
    "\n",
    "#\n",
    "# error handling\n",
    "#\n",
    "\n",
    "def p_error(p):\n",
    "    # In some cases, p could actually be None.\n",
    "    # However, stack trace should have enough information to identify the problem.\n",
    "    raise RuntimeError(\"Syntax error at '%s'\" % repr(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: test the parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: xpath examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
