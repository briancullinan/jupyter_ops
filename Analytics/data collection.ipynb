{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "\n",
    "Collect lots of things from lots of sources.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic http tools\n",
    "\n",
    "Copying files from many sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### copy list of links to json\n",
    "\n",
    "https://www.maricopacountyattorney.org/CivicAlerts.aspx?AID=400\n",
    "\n",
    "Crime reports?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/crimes');\n",
    "\n",
    "function scrapeAlert(ID) {\n",
    "    if(fs.existsSync(path.join(project, 'maricopa-alert-' + ID + '.json'))) {\n",
    "        return;\n",
    "    }\n",
    "    return client\n",
    "        .url('https://www.maricopacountyattorney.org/CivicAlerts.aspx?AID=' + ID)\n",
    "        .getAllXPath({\n",
    "            time: '//*[@class = \"single\"]//*[@class = \"date\"]//text()',\n",
    "            title: '//*[contains(@class, \"single\")]//h3//text()',\n",
    "            content: '//*[@class = \"single\"]//*[@class = \"content\"]//text()'\n",
    "        })\n",
    "        .then(r => {\n",
    "            fs.writeFileSync(path.join(project, 'maricopa-alert-' + ID + '.json'), JSON.stringify(r, null, 4));\n",
    "            return r;\n",
    "        })\n",
    "        .catch(e => console.log(e))\n",
    "}\n",
    "module.exports = scrapeAlert;\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    var IDs = Array.from(Array(500).keys());\n",
    "    multiCrawl(IDs, 'crime reports')\n",
    "        .then(r => $$.sendResult(r))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TODO: download amazon history\n",
    "\n",
    "Because Amazon download offer it as a download like Netflix does.\n",
    "\n",
    "https://www.amazon.com/gp/yourstore/iyr/ref=pd_ys_iyr_next?ie=UTF8&collection=watched&iyrGroup=&maxItem=616&minItem=600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search a lot of engines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code \n",
    "\n",
    "meta search all?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "var importer = require('../Core');\n",
    "var multiCrawl = importer.import('multi crawl');\n",
    "\n",
    "// http://www.exploratorium.edu/files/ronh/research/index.html\n",
    "/*\n",
    "\n",
    " Exploratorium\t\n",
    " Search\n",
    " Google\t\n",
    " Search\n",
    " alltheweb\t\n",
    " Search\n",
    " Teoma\t\n",
    " Search\n",
    " AltaVista\t\n",
    " Search\n",
    " Wisenut\t\n",
    " Search\n",
    " HotBot\t\n",
    " Search\n",
    " lii.org\t\n",
    " Search\n",
    " Northern Light\t\n",
    " Search\n",
    " Lycos\t\n",
    " Search\n",
    " Scirus\t\n",
    " \n",
    " Meta:\n",
    "  Ask Jeeves\t\n",
    " Search\n",
    " MetaCrawler\t\n",
    " Search\n",
    " Dogpile\t\n",
    " Search\n",
    " SavvySearch\t\n",
    " \n",
    " \n",
    " */\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/searches');\n",
    "\n",
    "function searchAll(query = 'search engine') {\n",
    "    var engines = [\n",
    "        'https://www.google.com/search?q=' + query,\n",
    "        'https://www.bing.com/search?q=' + query,\n",
    "        'https://search.yahoo.com/search?p=' + query,\n",
    "        'https://www.ask.com/web?q=' + query,\n",
    "        'https://search.aol.com/aol/search?q=' + query,\n",
    "        'http://www.baidu.com/s?wd=' + query,\n",
    "        'https://www.wolframalpha.com/input/?i=' + query,\n",
    "        'https://duckduckgo.com/?q=' + query,\n",
    "        'https://www.yandex.com/search/?text=' + query,\n",
    "        'https://archive.org/search.php?query=' + query,\n",
    "    ];\n",
    "    \n",
    "    // TODO: save results\n",
    "    return multiCrawl(engines, 'search results json')\n",
    "        .then(r => {\n",
    "            const time = new Date();\n",
    "            fs.writeFileSync(path.join(project, query.replace(/[^a-z0-9]/ig, '_')\n",
    "                                       + '-' + time.getFullYear()\n",
    "                                       + '-' + (time.getMonth() + 1)\n",
    "                                       + '-' + time.getDate()\n",
    "                                       + '.json'), JSON.stringify(r, null, 4));\n",
    "            return r;\n",
    "        })\n",
    "}\n",
    "module.exports = searchAll;\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    searchAll()\n",
    "        .then(r => $$.sendResult(r))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### schedule search all?\n",
    "\n",
    "TODO: convert this to a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var google = require('googleapis');\n",
    "var importer = require('../Core');\n",
    "var {\n",
    "    getOauthClient,\n",
    "    ISODateString,\n",
    "    createNewEvent,\n",
    "} = importer.import([\n",
    "    'convert date iso',\n",
    "    'create new calendar event',\n",
    "    'import google calendar api',\n",
    "]);\n",
    "\n",
    "var options = {\n",
    "    calendarId: 'aws'\n",
    "}\n",
    "\n",
    "function scheduleSearch(search) {\n",
    "    const parameters = {\n",
    "        query: search || 'search engines'\n",
    "    }\n",
    "    const newDate = new Date();\n",
    "    return (typeof options.auth === 'undefined'\n",
    "           ? getOauthClient(options)\n",
    "           : Promise.resolve([]))\n",
    "        .then(() => createNewEvent('meta search all', JSON.stringify(parameters, null, 4), options))\n",
    "}\n",
    "module.exports = scheduleSearch;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### tell joke?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var util = require('bluebird');\n",
    "var request = util.promisify(require('request'));\n",
    "var importer = require('../Core')\n",
    "\n",
    "var jokes;\n",
    "function getJoke() {\n",
    "    // TODO: collect jokes instead\n",
    "    return (typeof jokes === 'undefined'\n",
    "        ? request('http://www.ducksters.com/jokes/silly.php')\n",
    "        .then(res => importer.regexToArray(/^.*?Q:.*$|^.*?A:.*$/igm, res.body))\n",
    "        .then(r => {\n",
    "            r = r.reduce((arr, j, i) => {\n",
    "                if(i % 2 === 1) {\n",
    "                    arr.push([\n",
    "                        r[i-1].replace(/<.*?\\s*\\/?>/ig, '').trim().replace(/^\\s*|\\s*$/igm, ''),\n",
    "                        j.replace(/<.*?\\s*\\/?>/ig, '').trim().replace(/^\\s*|\\s*$/igm, '')\n",
    "                    ]);\n",
    "                }\n",
    "                return arr;\n",
    "            }, []);\n",
    "            console.log(r);\n",
    "            jokes = r;\n",
    "            return r;\n",
    "        })\n",
    "        : Promise.resolve(jokes))\n",
    "        .then(arr => {\n",
    "            const i = Math.round(Math.random() * arr.length);\n",
    "            return arr[i];\n",
    "        })\n",
    "}\n",
    "module.exports = getJoke;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advanced http tools \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TODO multicrawl with selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n",
    "multi crawl?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var importer = require('../Core');\n",
    "var runSeleniumCell = importer.import('selenium cell');\n",
    "\n",
    "var TIMEOUT = 1000;\n",
    "var CONNECTIONS = 3;\n",
    "\n",
    "// recursively dequeue tasks\n",
    "function deQueue(inputQueue, searchCallback, ctx) {\n",
    "    const results = [];\n",
    "    console.log(ctx.client.requestHandler.sessionID);\n",
    "    const callback = typeof searchCallback === 'function'\n",
    "        ? searchCallback\n",
    "        : importer.import(searchCallback, Object.assign({useCache: false}, ctx));\n",
    "    if(inputQueue.length > 0) {\n",
    "        const item = inputQueue.shift();\n",
    "        return new Promise(resolve => setTimeout(() => resolve(), 100))\n",
    "            .then(() => callback(...[item, ctx]))\n",
    "            .catch(e => {\n",
    "                console.log(e + '');\n",
    "                if((e + '').indexOf('Already') > -1 || (e + '').indexOf('session') > -1) {\n",
    "                    inputQueue.push(item);\n",
    "                    throw new Error('Abandoning session :(', e);\n",
    "                }\n",
    "            })\n",
    "            .then(r => results.push(r))\n",
    "            .then(() => deQueue(inputQueue, searchCallback, ctx))\n",
    "            .then(r => results.concat(r))\n",
    "            .catch(e => console.log(e))\n",
    "    } else {\n",
    "        return results;\n",
    "    }\n",
    "}\n",
    "\n",
    "// create a number of individual selenium sessions and dequeue the tasks with the callback search\n",
    "function multiCrawl(inputList, searchCallback) {\n",
    "    var indexes = Array.from(Array(Math.min(inputList.length, CONNECTIONS)).keys());\n",
    "    var connections = [];\n",
    "    var promises = indexes.map((s, i) => resolve => {\n",
    "        const client = runSeleniumCell(false, false);\n",
    "        return client\n",
    "            // skip this if error\n",
    "            //.then(() => connections[i].onlyOneWindow())\n",
    "            //.then(() => connections[i].resizeWindow())\n",
    "            .then(ctx => {\n",
    "                connections.push(ctx);\n",
    "                resolve(ctx)\n",
    "            })\n",
    "            .catch(e => {\n",
    "                console.log(e);\n",
    "                resolve(null);\n",
    "            })\n",
    "    });\n",
    "    var queue = [].concat(inputList);\n",
    "    var count = 0;\n",
    "    return importer.runAllPromises(promises)\n",
    "        .then(() => {\n",
    "            console.log(connections.map(c => c.client.requestHandler.sessionID));\n",
    "        })\n",
    "        .then(() => {\n",
    "            return connections[0].client\n",
    "                .scanning(true)\n",
    "                .then(() => connections[0].getAllSessionUrls())\n",
    "                .scanning(false)\n",
    "        })\n",
    "        .then(() => console.log('done loading sessions'))\n",
    "        .then(() => Promise.all(connections.map(ctx => deQueue(queue, searchCallback, ctx))))\n",
    "        .then(r => [].concat([], ...r))\n",
    "}\n",
    "module.exports = multiCrawl;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl domain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "var importer = require('../Core');\n",
    "var {doBrowserRequest} = importer.import('browser crawler tools')\n",
    "var {\n",
    "    cacheFilename,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "    rmhash\n",
    "} = importer.import('domain cache tools')\n",
    "\n",
    "async function crawlRecursive(url, depth, searches) {\n",
    "    if(!depth) depth = 3 // TODO: minutes depth using time range?\n",
    "    url = (typeof url === 'string' ? [url] : url )\n",
    "    // searches2 keeps track of new pages that should be added if searches is not provided\n",
    "    //   this guaruntees at least one page will be requested when this is called\n",
    "    const searches2 = []\n",
    "    for(var i = 0; i < url.length; i++) {\n",
    "        var l = url[i]\n",
    "        try {\n",
    "            await doBrowserRequest(l, readCache.bind(null, searches || searches2),\n",
    "                                   storeCache.bind(null, searches || searches2))\n",
    "        } catch (e) {\n",
    "            console.log(e)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // TODO: fix this\n",
    "    \n",
    "    // push old cache on to bottom of current searches,\n",
    "    //   so we are always getting at least the page requested\n",
    "    if(!searches) searches = searches2.concat(existingCache(url[0]))\n",
    "    \n",
    "    var existing = searches.map(s => rmhash(s.url))\n",
    "    var links = searches2\n",
    "    // TODO: pattern defensive programming\n",
    "        .map(s => {\n",
    "            var styles = s.styles || []\n",
    "            var links = s.links || []\n",
    "            return styles.concat(links)\n",
    "        })\n",
    "        .flat()\n",
    "        // do not include hash in actual link to the page\n",
    "        .map(s => rmhash(s))\n",
    "        // filter out first occurence\n",
    "        .filter((l, i, arr) => arr.indexOf(l) === i\n",
    "        // filter out all existing urls\n",
    "                && !existing.includes(rmhash(l))\n",
    "        // filter out data uris\n",
    "                && !l.includes('data:') && !l.includes('mailto:')\n",
    "                && !l.includes('javascript:') && !l.includes('ios-app:'))\n",
    "    \n",
    "    if(depth > 1) {\n",
    "        return await crawlRecursive(links, depth - 1, searches)\n",
    "    }\n",
    "    \n",
    "    // close the browser\n",
    "    await doBrowserRequest(false)\n",
    "    \n",
    "    // save the database\n",
    "    var filePath = cacheFilename(searches[0] ? searches[0].url : url[0])\n",
    "    fs.writeFileSync(filePath, JSON.stringify(searches, null, 2))\n",
    "}\n",
    "\n",
    "async function crawlAll(url, depth, searches) {\n",
    "    try {\n",
    "        await crawlRecursive(url, depth, searches)\n",
    "    } catch (e) {\n",
    "        console.log(e)\n",
    "        await doBrowserRequest(false)\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = crawlAll\n",
    "\n",
    "//var importer = require('../Core')\n",
    "//var crawlAll = importer.import('crawl domain')\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async()\n",
    "    crawlAll('https://google.com', 2)\n",
    "        .then(r => $$.sendResult('done'))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### domain cache tools?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs')\n",
    "var path = require('path')\n",
    "var importer = require('../Core')\n",
    "var {glob} = importer.import('glob files')\n",
    "var {getResponseContent} = importer.import('browser crawler tools')\n",
    "\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/crawls');\n",
    "\n",
    "function cacheFilename(url) {\n",
    "    if(typeof url === 'string')\n",
    "        url = new URL(url.includes('://') ? url : ('http://' + url))\n",
    "    const time = new Date()\n",
    "    var file = safeurl(url.hostname)\n",
    "       + '-' + time.getFullYear()\n",
    "       + '-' + (time.getMonth() + 1)\n",
    "       + '-' + time.getDate() + '.json'\n",
    "    return path.join(project, file)\n",
    "}\n",
    "\n",
    "function findCache(url) {\n",
    "    if(typeof url === 'string') {\n",
    "        url = new URL(url.includes('://') ? url : ('http://' + url))\n",
    "    }\n",
    "    const host = safeurl(url.hostname)\n",
    "    const crawl = glob(\n",
    "        '**/*' + host + '*',\n",
    "        project\n",
    "    )\n",
    "    crawl.sort((a, b) => {\n",
    "        return fs.statSync(b).mtime.getTime() - fs.statSync(a).mtime.getTime()\n",
    "    })\n",
    "    return crawl\n",
    "}\n",
    "\n",
    "function existingCache(url) {\n",
    "    var cache = findCache(url)\n",
    "    var filePath = cacheFilename(url)\n",
    "    // save pages from the same day in the same database using the url as the keys\n",
    "    if(cache[0] === filePath) {\n",
    "        return JSON.parse(fs.readFileSync(filePath)) || []\n",
    "    }\n",
    "    return []\n",
    "}\n",
    "\n",
    "async function storeCache(cache, response) {\n",
    "    var headers = await response.headers()\n",
    "    var result = await getResponseContent(response, headers)\n",
    "    console.log('Received ' + result.url)\n",
    "    cache.push(result)\n",
    "}\n",
    "\n",
    "async function readCache(cache, request) {\n",
    "    const url = (await request.url()).toLowerCase()\n",
    "    var response = cache.filter(s => s.url.toLowerCase().localeCompare(url) === 0)[0]\n",
    "    if (response && response.expires\n",
    "        && response.expires > Date.now()) {\n",
    "        await request.respond({\n",
    "            body: response.content,\n",
    "            contentType: response.type\n",
    "        })\n",
    "        return\n",
    "    }\n",
    "    request.continue();\n",
    "}\n",
    "\n",
    "// TODO: move this to URL tools in Languages/html.ipynb with getAllLinks\n",
    "function rmhash(url) {\n",
    "    return url.replace(/#.*$/ig, '')\n",
    "}\n",
    "\n",
    "// TODO: replace other occurrences with this function\n",
    "function safeurl(url) {\n",
    "    return url.replace(/[^a-z0-9_-]/ig, '_').substr(0, 100)\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "    cacheFilename,\n",
    "    findCache,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "    rmhash,\n",
    "    safeurl,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### browser crawler tools?\n",
    "\n",
    "Partly derrived from https://help.apify.com/en/articles/2424032-cache-responses-in-puppeteer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var puppeteer = require('puppeteer')\n",
    "var importer = require('../Core')\n",
    "var {selectDom} = importer.import('select tree')\n",
    "\n",
    "// TODO: make a utility for this because it keeps coming up here and in convert spreadsheet\n",
    "function getStyleUrls(content, url) {\n",
    "    return importer.regexToArray(/url\\s*\\(['\"]*([^\\)]*?)['\"]*\\)/ig, content, 1)\n",
    "        .map(l => new URL(l, url).href)\n",
    "}\n",
    "\n",
    "// TODO: make a utility for this because it keeps coming up here and in convert spreadsheet\n",
    "function getAllLinks(url, doc) {\n",
    "    var r = selectDom({\n",
    "        links: ['//a/@href|(//img|//iframe|//audio)[@src]/@src'],\n",
    "        styles: ['//link/@href|//style/@text()'],\n",
    "        html: ['/*']\n",
    "    }, doc)\n",
    "    var html = r.html.map(h => h.outerHTML).join('')\n",
    "    return {\n",
    "        links: r.links.map(s => new URL(s, url).href)\n",
    "            .concat(getStyleUrls(html, url)),\n",
    "        styles: r.styles.map(s => new URL(s, url).href),\n",
    "        html: html\n",
    "    }\n",
    "}\n",
    "\n",
    "async function getExpires(headers) {\n",
    "    const cacheControl = headers['cache-control'] || '';\n",
    "    const maxAgeMatch = cacheControl.match(/max-age=(\\d+)/);\n",
    "    const maxAge = maxAgeMatch && maxAgeMatch.length > 1 ? parseInt(maxAgeMatch[1], 10) : 0;\n",
    "    return Date.now() + (maxAge * 1000)\n",
    "}\n",
    "\n",
    "async function getResponseContent(response, headers) {\n",
    "    if(typeof headers['content-type'] === 'undefined') {\n",
    "        console.log(headers)\n",
    "    }\n",
    "    var type = (headers['content-type'] || '').split(';')[0]\n",
    "    var length = 0\n",
    "    try {\n",
    "        length = parseInt(headers['content-length'])\n",
    "    } catch (e) {}\n",
    "    var result = {\n",
    "        url: await response.url(),\n",
    "        type: type,\n",
    "        length: length,\n",
    "        expires: await getExpires(headers),\n",
    "    }\n",
    "    // check headers and make sure we don't download too much\n",
    "    if(!length || length < 100*1024*1024) {\n",
    "        try {\n",
    "            var buffer = await response.buffer()\n",
    "            await new Promise(resolve => setTimeout(resolve, 100))\n",
    "            if(result.type.includes('text/')) {\n",
    "                // TODO replace with encoding from header\n",
    "                result.content = buffer.toString('utf8') \n",
    "            } else {\n",
    "                result.content = `data:${type};base64,${buffer.toString('base64')}`\n",
    "            }\n",
    "            if(result.type.includes('/html')) {\n",
    "                Object.assign(result, getAllLinks(result.url, result.content))\n",
    "            }\n",
    "        } catch (e) {\n",
    "            if(e.message.includes('Target closed')\n",
    "              || e.message.includes('Response body is unavailable')\n",
    "              || e.message.includes('Protocol error')) {\n",
    "                console.log(`Probable tracker ${result.url}`)\n",
    "            } else {\n",
    "                throw e\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "var browser\n",
    "var RETRY_COUNT = 1\n",
    "async function doBrowserRequest(url, readCache, storeCache, callback, retry = 0) {\n",
    "    if(url === false) {\n",
    "        await browser.close()\n",
    "        browser = null\n",
    "        return\n",
    "    }\n",
    "    if(!browser) browser = await puppeteer.launch()\n",
    "    const page = await browser.newPage()\n",
    "    await page.setRequestInterception(true)\n",
    "    page.on('request', readCache)\n",
    "    page.on('response', storeCache)\n",
    "    var result\n",
    "    try {\n",
    "        result = await page.goto(url)\n",
    "    } catch (e) {\n",
    "        if(retry < RETRY_COUNT) {\n",
    "            await doBrowserRequest(false)\n",
    "            return await doBrowserRequest(url, readCache, storeCache, callback, retry+1)\n",
    "        } else if(e.message.includes('ERR_NAME_NOT_RESOLVED')) {\n",
    "            console.log(`Could not fetch ${url}`)\n",
    "        } else {\n",
    "            throw e\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if(callback) await callback(result, page)\n",
    "    await page.close()\n",
    "    return result\n",
    "}\n",
    "    \n",
    "module.exports = {\n",
    "    getAllLinks,\n",
    "    getResponseContent,\n",
    "    doBrowserRequest\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze cache file?\n",
    "\n",
    "Show some interesting information about cache files, and remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs')\n",
    "var {findCache} = importer.import('domain crawler tools')\n",
    "\n",
    "function analyzeCache(url) {\n",
    "    var cache = findCache(url)\n",
    "    if(cache.length === 0) {\n",
    "        return {\n",
    "            error: `No cache file found ${url}`\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    var json = JSON.parse(fs.readFileSync(cache[0]))\n",
    "    var domains = json.map(s => new URL(s.url).hostname)\n",
    "        .filter((h, i, arr) => arr.indexOf(h) === i)\n",
    "    var largeness = json.sort((a, b) => b.content.length - a.content.length)\n",
    "        .slice(0, 10)\n",
    "    var urls = json.map(s => s.url)\n",
    "    var repeats = json.filter((s, i, arr) => i > 0 && urls.indexOf(s.url) === i)\n",
    "    fs.writeFileSync(cache[0], JSON.stringify(repeats, null, 2))\n",
    "    return {\n",
    "        countPages: json.length,\n",
    "        countCaches: cache.length,\n",
    "        target: json[0].url,\n",
    "        countDomains: domains.length,\n",
    "        domains: domains,\n",
    "        countLargest: largeness.reduce((cur, l) => cur + l.content.length, 0),\n",
    "        largest10: largeness.map(l => l.url),\n",
    "        repeats: json.length - repeats.length,\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = analyzeCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### schedule crawl domain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var google = require('googleapis');\n",
    "var importer = require('../Core');\n",
    "var {\n",
    "    getOauthClient,\n",
    "    ISODateString,\n",
    "    createNewEvent,\n",
    "} = importer.import([\n",
    "    'convert date iso',\n",
    "    'create new calendar event',\n",
    "    'import google calendar api',\n",
    "]);\n",
    "\n",
    "var options = {\n",
    "    calendarId: 'aws'\n",
    "}\n",
    "\n",
    "function scheduleSearch(search) {\n",
    "    const parameters = {\n",
    "        query: search\n",
    "    }\n",
    "    const newDate = new Date();\n",
    "    return (typeof options.auth === 'undefined'\n",
    "           ? getOauthClient(options)\n",
    "           : Promise.resolve([]))\n",
    "        .then(() => createNewEvent('crawl domain', JSON.stringify(parameters, null, 4), options))\n",
    "}\n",
    "module.exports = scheduleSearch;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect bookmarks pages\n",
    "\n",
    "Convert all bookmarks to PDF and store in a JSON database for wiring up to express or my editor page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### introduction\n",
    "\n",
    "One of the most irritating and terrifying things about the modern web is losing information systems depend on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've experienced this, pretty much anytime some links to a Microsoft help doc without a title. They don't copy the original point of the article, or even summerize, this makes it impossible to learn. Humans are a lossy species, we lose information all the time, and perhaps it is part of evolution. I on the other hand, have gotten increasingly good at keeping information. \n",
    "\n",
    "What if some instruction to fix my computer suddenly disappears, should my computer become useless because of it.\n",
    "What if I learn something that becomes poigently relevant sometime in the future but the link is gone.\n",
    "\n",
    "We go one link deep, download all dependencies, and use Phantom browser to do it. It also makes a PDF so the content is preserved in different forms.\n",
    "\n",
    "Additionally, the Bookmarks Manager in Chrome can't search page content for relevance, this is something we can fix using \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "collect all bookmarks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var path = require('path')\n",
    "var importer = require('../Core')\n",
    "var getBookmarksFromTakeout = importer.import('parse bookmarks file')\n",
    "var ISODateString = importer.import('convert date iso')\n",
    "var crawlAll = importer.import('crawl domain')\n",
    "var {doBrowserRequest} = importer.import('browser crawler tools')\n",
    "var {\n",
    "    safeurl,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "} = importer.import('domain cache tools')\n",
    "\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/pdfs');\n",
    "\n",
    "async function savePdf(filename, result, page) {\n",
    "    const pdf = await page.pdf({ format: 'A4' })\n",
    "    fs.writeFileSync(filename, pdf)\n",
    "}\n",
    "\n",
    "async function collectAllBookmarks() {\n",
    "    var folders = getBookmarksFromTakeout()\n",
    "    var links = folders.reduce(function flattenFolders(arr, cur) {\n",
    "        if(cur.folder === 'Sad Examples') return arr\n",
    "        arr.push.apply(arr, cur.links.concat(cur.children.reduce(flattenFolders, [])))\n",
    "        return arr\n",
    "    }, [])\n",
    "    for(var i = 0; i < links.length; i++) {\n",
    "        const cache = existingCache(links[i].url)\n",
    "        const filename = path.join(project, safeurl(links[i].url) + '.pdf')\n",
    "        \n",
    "        // check if there is a recent pdf and skip\n",
    "        if(fs.existsSync(filename)) continue\n",
    "        await crawlAll(links[i].url, 1, cache)\n",
    "\n",
    "        // save a pdf\n",
    "        try {\n",
    "            await doBrowserRequest(links[i].url, \n",
    "                                   readCache.bind(null, cache),\n",
    "                                   storeCache.bind(null, cache),\n",
    "                                   savePdf.bind(null, filename))\n",
    "        } catch (e) {\n",
    "            console.log(e)\n",
    "            await doBrowserRequest(false)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = collectAllBookmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## html tools\n",
    "\n",
    "TODO: move this to Languages/html.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### find search boxes on a page\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n",
    "search results as json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function searchResultsToJson(url) {\n",
    "    console.log(url + ' - ' + client.requestHandler.sessionID);\n",
    "    return client\n",
    "        .url(url)\n",
    "        .pause(2000)\n",
    "        .getAllXPath({\n",
    "            query: '//input[contains(@aria-label, \"Search\")]/@value'\n",
    "            +\n",
    "            '|//input[contains(@aria-label, \"search\")]/@value'\n",
    "            +\n",
    "            // yahoo\n",
    "            '|//label[contains(., \"Search\")]/following::*//input[@type=\"text\"]/@value' \n",
    "            +\n",
    "            '|//input[contains(@class, \"Search\")]/@value'\n",
    "            +\n",
    "            // wolfram\n",
    "            '|//input[contains(@name, \"query\")]/@value'\n",
    "            +\n",
    "            // duckduckgo\n",
    "            '|//input[contains(@id, \"search\")]/@value'\n",
    "            +\n",
    "            // yandex\n",
    "            '|//input[contains(@aria-label, \"Request\")]/@value',\n",
    "            results: [\n",
    "                '//h3|//h2|div[contains(@class, \"title\")]'\n",
    "                +\n",
    "                // ask\n",
    "                '|//*[contains(@class, \"item-title\")]',\n",
    "                {\n",
    "                    name: './/text()',\n",
    "                    summary: './/following-sibling::div//text()'\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        .then(r => {\n",
    "            return {\n",
    "                url: url,\n",
    "                query: typeof r.query === 'string'\n",
    "                    ? r.query\n",
    "                    : r.query[0],\n",
    "                results: r.results.map(s => ({\n",
    "                    name: typeof s.name === 'string'\n",
    "                        ? s.name : s.name.join('\\n'),\n",
    "                    summary: typeof s.summary === 'string'\n",
    "                        ? s.summary : s.summary.join('\\n')\n",
    "                }))\n",
    "            };\n",
    "        })\n",
    "        .catch(e => {\n",
    "            console.log(e)\n",
    "            return {\n",
    "                url: url,\n",
    "                query: null,\n",
    "                results: []\n",
    "            }\n",
    "        })\n",
    "}\n",
    "module.exports = searchResultsToJson;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "12.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
