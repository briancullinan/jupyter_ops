{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edit anywhere\n",
    "\n",
    "Purpose: change the web however you want, save and restore it from github, treat the web like one giant web of gists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction\n",
    "\n",
    "First attempt at this app was using megamind.bot app, the editor takes an input URL, and a filename.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It loads a website from crawled data from Selenium/data collection.ipynb. It then sent these parameters to the client app for displaying in the same frame.\n",
    "\n",
    "The new implementation should act more like a marketing website. Enter a URL in a box in the middle like Google, use the controls that appear on the copied page content. Use different methods to get the page content such as Selenium crawler or simple phantom browser.\n",
    "\n",
    "After the content is crawled, make any changes that save to a single Gist for the entire domain. Load the gist from domain when the page is loaded (plugin phase 2).\n",
    "\n",
    "TODO:\n",
    "\n",
    "Fix SPA/PWA apps and canvas copies.\n",
    "\n",
    "Move page processing to generalized Selenium, convert scripts.\n",
    "\n",
    "TODO: Use print to PDF in chrome ignoring print styles from Developer mode, better Selenium crawler.\n",
    "\n",
    "TODO: Make a tool for the page manipulation, this is a common theme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gist\n",
    "\n",
    "Read and write files from gist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read gist files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "read gist files?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Octokit = require('@octokit/rest');\n",
    "\n",
    "// commit changes to github\n",
    "function getGist(gist) {\n",
    "    if(!gist) return {}\n",
    "    const github = new Octokit({\n",
    "        host: 'api.github.com'\n",
    "    });\n",
    "    /*\n",
    "    github.authenticate({\n",
    "        type: 'basic',\n",
    "        username: process.env.USERNAME,\n",
    "        password: process.env.PASSWORD\n",
    "    });\n",
    "    */\n",
    "\n",
    "    //return github.gists.get({gist_id: gist})\n",
    "    return github.gists.get({gist_id: gist})\n",
    "        .then(r => r.data)\n",
    "        .catch(e => console.log(e))\n",
    "}\n",
    "\n",
    "module.exports = getGist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test gist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var importer = require('../Core');\n",
    "var getGist = importer.import('read gist files');\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    getGist('a572d0830ae72b962e12a57adaec7c52')\n",
    "        .then(r => $$.sendResult(r))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write gist files\n",
    "\n",
    "write gist files?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ckeditor client\n",
    "\n",
    "Use CKEditor and some scripts to apply some ACLs to the page and output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply ACL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "apply acl to html?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// scan using an acl list, similar to easylist?\n",
    "// TODO: accept formats:\n",
    "//    {\"selector\": \"selector\"}\n",
    "//    {\"glob-url@selector\": \"glob-template-path@selector\"}\n",
    "//    {\"selector\": \"html-file@selector\"}\n",
    "//    {\"selector\": \"html-file@xpath\"} ?\n",
    "//    {\"glob-file\": {\"glob-url\"...} || [\"selector\"]}\n",
    "const paths = JSON.parse('[]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load ckeditor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "load ckeditor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var importer = require('../Core')\n",
    "var loadScraped = importer.import('get scraped page')\n",
    "var getGist = importer.import('read gist files')\n",
    "var {selectDom} = importer.import('select tree')\n",
    "\n",
    "// git \n",
    "async function gitEditor(url, gist) {\n",
    "    // TODO: use a Github repo as the input\n",
    "    if(typeof url == 'string') {\n",
    "        url = new URL(url);\n",
    "    }\n",
    "    var file = url.pathname\n",
    "    var host = url.hostname.replace(/[^a-z0-9_-]/ig, '_')\n",
    "    if(!file) file = 'index'\n",
    "\n",
    "    var files = await loadScraped(url)\n",
    "    if(typeof files[ host + '-acl.json' ] === 'undefined') {\n",
    "        var saved = (await getGist(gist)).files\n",
    "        if(saved && saved[host + '-acl.json']) {\n",
    "            files[host + '-acl.json'] = JSON.parse(saved[host + '-acl.json'].content || '[]')\n",
    "            if(typeof files[host + '-acl.json'] === 'string') {\n",
    "                files[host + '-acl.json'] = [files[host + '-acl.json']]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // inject the editor into copied page\n",
    "    var doc = selectDom('*', files[file])\n",
    "    var body = selectDom('//body', doc)\n",
    "    console.log(body.append)\n",
    "    if(body) {\n",
    "        //body.append(selectDom('//script', '<script src=\"https://cdn.ckeditor.com/ckeditor5/16.0.0/classic/ckeditor.js\"></script>'))\n",
    "        // add content editable to -acl list elements\n",
    "        (files[host + '-acl.json'] || []).forEach(i => {\n",
    "            var els = selectDom([i], body)\n",
    "            els.forEach(el => {\n",
    "                el.setAttribute('contenteditable', 'contenteditable')\n",
    "            })\n",
    "        })\n",
    "        console.log('done')\n",
    "        return body.ownerDocument.documentElement.outerHTML\n",
    "    } else {\n",
    "        throw Error(`Not found ${url}`)\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = gitEditor\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    gitEditor('https://www.google.com')\n",
    "        .then(r => $$.mime({'text/html': r}))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restrain CSS\n",
    "\n",
    "Replace all CSS rules with a container ID to restain it's affects on the page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "restrain css? \n",
    "\n",
    "scope css?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var css = require('css');\n",
    "\n",
    "function prefixCssRules(str, prefix) {\n",
    "    try {\n",
    "        const ast = css.parse(str);\n",
    "        // TODO: add a check for media queries\n",
    "        ast.stylesheet.rules.forEach(r => {\n",
    "            if(typeof r.selectors === 'undefined') {\n",
    "                return;\n",
    "            }\n",
    "            r.selectors.forEach((s, i) => {\n",
    "                if(s.includes('body')) {\n",
    "                    r.selectors[i] = s.replace(/\\s*body\\s*/ig, prefix);\n",
    "                } else {\n",
    "                    r.selectors[i] = prefix + ' ' + s;\n",
    "                }\n",
    "            });\n",
    "        })\n",
    "        return css.stringify(ast);\n",
    "    } catch (e) {\n",
    "        console.log(e)\n",
    "        return str\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "module.exports = prefixCssRules;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TODO: express crawl middleware\n",
    "\n",
    "Serve every static address from a cache crawl json file.\n",
    "\n",
    "TODO: move this to data collection tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read crawl files\n",
    "\n",
    "Load matching files from a crawled cache json file. See Selenium/data collection.ipynb for more information on crawl cache json.\n",
    "\n",
    "TODO: move this to data collection.ipynb tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code \n",
    "\n",
    "read crawl files?\n",
    "\n",
    "get scraped page?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var path = require('path')\n",
    "var fs = require('fs')\n",
    "var {URL} = require('url')\n",
    "var uuid = require('uuid/v1')\n",
    "var importer = require('../Core')\n",
    "var {glob} = importer.import('glob files')\n",
    "var {minimatch} = importer.import('minimatch')\n",
    "var {selectDom} = importer.import('select tree')\n",
    "var prefixCssRules = importer.import('scope css')\n",
    "var {findCache} = importer.import('domain crawler tools')\n",
    "\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/crawls');\n",
    "\n",
    "function matchPage(match, search, hostname) {\n",
    "    return search.includes(match)\n",
    "        || minimatch(search, match)\n",
    "        || (!match || match === 'index')\n",
    "        && search.match(/https?:\\/\\/[^\\/]*\\/?$/ig)\n",
    "        && search.includes(hostname)\n",
    "}\n",
    "\n",
    "function loadScraped(url) {\n",
    "    if(typeof url == 'string') {\n",
    "        url = new URL(url);\n",
    "    }\n",
    "    //console.log(url)\n",
    "    var host = url.hostname\n",
    "    var file = url.pathname\n",
    "    if(!file) file = 'index'\n",
    "    \n",
    "    // lookup on filesystem\n",
    "    var cache = findCache(host)\n",
    "    if(!cache[0]) {\n",
    "        return\n",
    "    }\n",
    "    const crawl = JSON.parse(fs.readFileSync(cache[0]).toString());\n",
    "    const entry = crawl.filter(r => matchPage(file, r.url, host))[0];\n",
    "    const result = {}\n",
    "    //console.log(entry)\n",
    "    // parse out styles and images and package it up in to one nice page\n",
    "    if(entry) {\n",
    "        const HOST_ID = host.replace(/[^a-z0-9_-]/ig, '_')\n",
    "        var doc = selectDom('*', entry.html)\n",
    "        var styles = selectDom(['//link[@rel = \"stylesheet\"]|//style'], doc)\n",
    "        var css = ''\n",
    "        styles.forEach(s => {\n",
    "            var src = s.getAttribute('src') || s.getAttribute('href')\n",
    "            s.remove()\n",
    "            if(!src) {\n",
    "                css += s.innerHTML\n",
    "                return\n",
    "            }\n",
    "            src = new URL(src, url).href\n",
    "            var rules = crawl.filter(r => r.url === src)[0]\n",
    "            if(rules) {\n",
    "                css += rules.content\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        var scripts = selectDom(['//script'], doc)\n",
    "        scripts.forEach(s => s.remove())\n",
    "        \n",
    "        var images = selectDom(['//img'], doc)\n",
    "        images.forEach(i => {\n",
    "            var src = i.getAttribute('src')\n",
    "            src = new URL(src, url).href\n",
    "            var images = crawl.filter(r => r.url === src)[0]\n",
    "            if(images && images.content.includes('data:')) {\n",
    "                i.setAttribute('src', images.content)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        var links = selectDom(['//a'], doc)\n",
    "        links.forEach(l => {\n",
    "            var src = l.getAttribute('href')\n",
    "            src = new URL(src, url).href\n",
    "            l.setAttribute('href', '/?url=' + src)\n",
    "        })\n",
    "        \n",
    "        // TODO: load images as data URIs and lower quality\n",
    "        css = prefixCssRules(css, '#' + HOST_ID)\n",
    "        .replace(/url\\s*\\(['\"]*([^\\)]*?)['\"]*\\)/ig, ($0, $1) => {\n",
    "            var src = new URL($1, url).href\n",
    "            var images = crawl.filter(r => r.url === src)[0]\n",
    "            if(images && images.content.includes('data:')) {\n",
    "                return `url(${images.content})`\n",
    "            }\n",
    "            return $0\n",
    "        })\n",
    "        .replace(/href=\"([^\\\"]*)\"/ig, ($0, $1) => {\n",
    "            var src = new URL($1, url).href\n",
    "            return $0.replace($1, '/?url=' + src)\n",
    "        })\n",
    "        var body = selectDom('//body', doc)\n",
    "        var classes = body.getAttribute('class')\n",
    "        result[file] = `<html><head><style>\n",
    "body, html {\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "}\n",
    "</style><style>${css}</style>\n",
    "</head><body><div id=\"${HOST_ID}\" class=\"${classes}\">${body.innerHTML}</div>\n",
    "<script>\n",
    "var script = document.createElement('script')\n",
    "script.onload = function () {\n",
    "    var editors = document.querySelectorAll( '*[contenteditable]' )\n",
    "    editors.forEach(e => {\n",
    "        InlineEditor\n",
    "        .create( e )\n",
    "        .catch( error => console.error( error ) )\n",
    "    })\n",
    "}\n",
    "script.setAttribute('src', 'https://cdn.ckeditor.com/ckeditor5/16.0.0/inline/ckeditor.js')\n",
    "document.body.appendChild(script)\n",
    "</script>\n",
    "</body></html>`\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "module.exports = loadScraped\n",
    "\n",
    "//var importer = require('../Core')\n",
    "//var loadScraped = importer.import('read crawl files')\n",
    "\n",
    "if(typeof $$ != 'undefined') {\n",
    "    var scraped = loadScraped('https://google.com')\n",
    "    $$.html(scraped)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test crawl cache loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "12.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
